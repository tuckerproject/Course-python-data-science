{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"L10qs.ipynb","provenance":[],"collapsed_sections":[]},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"GKvM7xzU61K_","colab_type":"text"},"source":["Naive Bayes, a linear classifier (a neural network with no hidden units and linear activation), and now logistic regression are similar in that they all learn a one-weight-per-feature model. How do they compare on the MNIST digit data from Lesson 3 on neural networks? Which does the best job of combining evidence from across the image to make a classification? Run the algorithms to find out."]},{"cell_type":"markdown","metadata":{"id":"Nt4XRcsM5wR0","colab_type":"text"},"source":["Import libraries and read in the data."]},{"cell_type":"code","metadata":{"id":"DvxEMcmmkBpQ","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593380377180,"user_tz":240,"elapsed":31381,"user":{"displayName":"Michael Littman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh1OI1zp1NevMSZ87UN47k8bTZ--Ijr4v3ORV33Vg=s64","userId":"06751891446459829367"}}},"source":["# Read in the mnist digit dataset\n","\n","from sklearn.datasets import fetch_openml\n","from sklearn.model_selection import train_test_split\n","from sklearn.utils import check_random_state\n","import random\n","from sklearn import tree\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from sklearn.neural_network import MLPClassifier\n","\n","X, y = fetch_openml('mnist_784', version=1, return_X_y=True)"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_GHdK3NLkK5s","colab_type":"text"},"source":["Next, we will divide the data into a training set and test set, randomly selecting 5000 examples for training"]},{"cell_type":"code","metadata":{"id":"PZUx3PuTkBsA","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593380377829,"user_tz":240,"elapsed":30783,"user":{"displayName":"Michael Littman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh1OI1zp1NevMSZ87UN47k8bTZ--Ijr4v3ORV33Vg=s64","userId":"06751891446459829367"}}},"source":["train_samples = 5000\n","\n","random_state = check_random_state(0)\n","permutation = random_state.permutation(X.shape[0])\n","X = X[permutation]\n","y = y[permutation]\n","X = X.reshape((X.shape[0], -1))\n","\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, train_size=train_samples, test_size=10000)"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6IGpGkIa52yz","colab_type":"text"},"source":["Train a linear classifier for 10k steps."]},{"cell_type":"code","metadata":{"id":"MO1EX5PZ3sy0","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1593380995136,"user_tz":240,"elapsed":3473,"user":{"displayName":"Michael Littman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh1OI1zp1NevMSZ87UN47k8bTZ--Ijr4v3ORV33Vg=s64","userId":"06751891446459829367"}},"outputId":"4a435f53-4bc7-438f-ae0c-6c9acbd0d8a4"},"source":["clf = MLPClassifier(hidden_layer_sizes=[], activation='identity', max_iter = 10000)\n","clf.fit(X_train, y_train)\n","score = clf.score(X_test, y_test)\n","\n","print(score)"],"execution_count":7,"outputs":[{"output_type":"stream","text":["0.8526\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"uBo0nEM75-uz","colab_type":"text"},"source":["Train a logistic regression for 10k steps."]},{"cell_type":"code","metadata":{"id":"2lfGMp257QCi","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1593352758311,"user_tz":240,"elapsed":268052,"user":{"displayName":"Michael Littman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh1OI1zp1NevMSZ87UN47k8bTZ--Ijr4v3ORV33Vg=s64","userId":"06751891446459829367"}},"outputId":"9b8bf786-17dd-4344-a169-1ef3dc23f8e8"},"source":["from sklearn.linear_model import LogisticRegression\n","\n","clf = LogisticRegression(max_iter = 10000)\n","clf.fit(X_train, y_train)\n","score = clf.score(X_test, y_test)\n","\n","print(score)"],"execution_count":11,"outputs":[{"output_type":"stream","text":["0.8655\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"nuWiOLMq6EoK","colab_type":"text"},"source":["Train Naive Bayes."]},{"cell_type":"code","metadata":{"id":"SQwM2Nie7zTZ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1593380662344,"user_tz":240,"elapsed":714,"user":{"displayName":"Michael Littman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh1OI1zp1NevMSZ87UN47k8bTZ--Ijr4v3ORV33Vg=s64","userId":"06751891446459829367"}},"outputId":"3fb103e7-a2d5-4c15-89d3-48d719525031"},"source":["from sklearn.naive_bayes import MultinomialNB\n","\n","clf = MultinomialNB()\n","clf.fit(X_train, y_train)\n","score = clf.score(X_test, y_test)\n","\n","print(score)"],"execution_count":6,"outputs":[{"output_type":"stream","text":["0.8221\n"],"name":"stdout"}]}]}
