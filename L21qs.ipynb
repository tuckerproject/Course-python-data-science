{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"L21qs.ipynb","provenance":[],"collapsed_sections":[]},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"sC0_ePr8ixp7","colab_type":"text"},"source":["Can you define a reward function that makes the grid-walking program go to the green square and then come back and end on a yellow square? What reward function does the inverse reinforcement-learning program return given that behavior as input? Does the reward function capture the right behavior?"]},{"cell_type":"markdown","metadata":{"id":"gN3jicMrRHiE","colab_type":"text"},"source":["We’ll imagine the decision maker in this world is a robot in a field. The field is a grid of 50 locations arranged in 5 rows by 10 columns. There are five different categories of locations. You can think of the categories as being types of terrain---say, gravel, grass, shallow water, mud, and asphalt. The robot can tell what category each location is. We can allocate different rewards to each category and then see how the robot navigates in response.\n","\n","Below we import some of the libraries we'll be using and build our grid for navigation, `grid_map`. The grid will have `nrows = 5` rows and `ncols = 10` columns.  The cells of the grid can grid will be one of `ncats = 5` possible values, from 0 to 4."]},{"cell_type":"code","metadata":{"id":"VeKJN9JdGHmD","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":170},"executionInfo":{"status":"ok","timestamp":1593945393467,"user_tz":240,"elapsed":6475,"user":{"displayName":"Michael Littman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh1OI1zp1NevMSZ87UN47k8bTZ--Ijr4v3ORV33Vg=s64","userId":"06751891446459829367"}},"outputId":"e45bdb9f-a2d4-410b-dc74-2323c22beac9"},"source":["!pip install keras=='2.3.1'\n","import numpy as np\n","import keras.backend as K\n"," \n","# the empty grid\n","nrows = 5\n","ncols = 10\n"," \n","# ncats is the number of state categories\n","ncats = 5\n"," \n","# map state categories to states\n","# want m s.t. r %*% m = reward function\n","# first, just a map of the indexes\n","grid_map = np.array([ [0,0,0,0,0,2,0,0,1,0],\n","                 [0,1,0,0,0,2,0,0,0,0],\n","                 [0,0,0,0,0,2,1,3,0,0],\n","                 [0,0,0,1,0,2,0,3,0,0],\n","                 [0,0,0,0,0,2,0,3,0,4]])"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: keras==2.3.1 in /usr/local/lib/python3.6/dist-packages (2.3.1)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras==2.3.1) (2.10.0)\n","Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras==2.3.1) (1.4.1)\n","Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras==2.3.1) (1.0.8)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras==2.3.1) (1.12.0)\n","Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras==2.3.1) (1.18.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras==2.3.1) (3.13)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras==2.3.1) (1.1.2)\n"],"name":"stdout"},{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"PaEkUMXJPtoG","colab_type":"text"},"source":["Next we'll visualize the map, assigning colors in the order of the `colors` list with `seaborn`:\n","0. white\n","1. blue\n","2. orange\n","3. yellow\n","4. green"]},{"cell_type":"code","metadata":{"id":"tVqK8K1zGJI-","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":319},"executionInfo":{"status":"ok","timestamp":1590067245417,"user_tz":240,"elapsed":984,"user":{"displayName":"Michael Littman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh1OI1zp1NevMSZ87UN47k8bTZ--Ijr4v3ORV33Vg=s64","userId":"06751891446459829367"}},"outputId":"23a0b7b8-f50c-42da-d751-d10fbb9c299a"},"source":["import seaborn as sns\n"," \n","colors = [\"white\", \"blue\", \"orange\", \"yellow\", \"green\"]\n"," \n","sns.heatmap(grid_map, cmap=sns.xkcd_palette(colors), yticklabels=False, xticklabels=False,\n","            annot=False, cbar = False, annot_kws={\"size\": 30}, linewidths=1, linecolor=\"gray\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n","  import pandas.util.testing as tm\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["<matplotlib.axes._subplots.AxesSubplot at 0x7fc60a4947f0>"]},"metadata":{"tags":[]},"execution_count":2},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAV0AAADnCAYAAAC9roUQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAEvElEQVR4nO3aQUolVxiG4XODU/eSDbgWZ5mkN6Agh4OgG+hMMnMtbiB7cQGViQEHVktD81ZBnmf6X+GjuL4U4mXbtgFA47ejBwD8n4guQEh0AUKiCxASXYDQ1Y+Oay3/2gDwk+acl73bD6P7/sO/ds1PWmsdvuMMG86y4wwbPu64e3s6bMPz9f0Y4zzP4gzfi8fX28M2jDHGw83LGOMcz2KPPy8AhEQXICS6ACHRBQiJLkBIdAFCogsQEl2AkOgChEQXICS6ACHRBQiJLkBIdAFCogsQEl2AkOgChEQXICS6ACHRBQiJLkBIdAFCogsQEl2AkOgChEQXICS6ACHRBQiJLkBIdAFCogsQEl2AkOgChEQXICS6ACHRBQiJLkBIdAFCogsQEl2AkOgChC7btu0e11r7RwA+Nee87N286QKErr76wJyz2LFrrTXGGOPx9fawDQ83L2OM8zyLI3ecYcPHHXdvT4dteL6+H2Oc51n4Xpxjx38b9njTBQiJLkBIdAFCogsQEl2AkOgChEQXICS6ACHRBQiJLkBIdAFCogsQEl2AkOgChEQXICS6ACHRBQiJLkBIdAFCogsQEl2AkOgChEQXICS6ACHRBQiJLkBIdAFCogsQEl2AkOgChEQXICS6ACHRBQiJLkBIdAFCogsQEl2AkOgChEQXICS6AKHLtm27x7XW/hGAT805L3s3b7oAoauvPjDnLHbsWmsdvuMMG86y4wwbPu64e3s6bMPz9f0YY4zH19vDNowxxsPNyxhjjDm/H7ZhrW/vG87xvTjD78geb7oAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAocu2bbvHtdb+EYBPzTkvezdvugChq68+MOcsduxaax2+4wwbPu54fL09bMPDzcsY4zzP4u7t6bANz9f3Y4zzPIs5vx+44dv7hrM8i+N7scebLkBIdAFCogsQEl2AkOgChEQXICS6ACHRBQiJLkBIdAFCogsQEl2AkOgChEQXICS6ACHRBQiJLkBIdAFCogsQEl2AkOgChEQXICS6ACHRBQiJLkBIdAFCogsQEl2AkOgChEQXICS6ACHRBQiJLkBIdAFCogsQEl2AkOgChEQXICS6ACHRBQhdtm3bPa619o8AfGrOedm7edMFCF199YE5Z7Fj11rr8B1n2HCWHWfY8HHH3dvTYRuer+/HGOd5FnN+P3DDt/cN53gWf/3+92Eb/vznjx/evekChEQXICS6ACHRBQiJLkBIdAFCogsQEl2AkOgChEQXICS6ACHRBQiJLkBIdAFCogsQEl2AkOgChEQXICS6ACHRBQiJLkBIdAFCogsQEl2AkOgChEQXICS6ACHRBQiJLkBIdAFCogsQEl2AkOgChEQXICS6ACHRBQiJLkBIdAFCogsQEl2AkOgChC7btu0e11r7RwA+Nee87N1+GF0Afi1/XgAIiS5ASHQBQqILEBJdgJDoAoT+BXoxxL/64QHtAAAAAElFTkSuQmCC\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"JPCU6BzbQGfE","colab_type":"text"},"source":["We can assign reward values, `r`, to each of the terrain categories.\n","\n","We will additionally binarize `grid_map`, adding an additional dimension representing the value in the cell to make `matmap`.  Values in `matmap` equal 1 if the value of row and column of that cell in `grid_map` equal is the value of the third dimension of the cell.  \n","\n","The `clip` function keeps the current location in the grid to be within the size of the grid. \n","\n","We define five actions, `acts`, in terms of how each one changes the robot’s row and column. The first decreases the row by 1 and leaves the column unchanged. That corresponds to moving UP in the grid. The other actions correspond to moving right, down, left, and staying in place.\n","\n","We will also build a transition matrix, `mattrans`, that describes the transition. This matrix represents the probability that the given action will cause a one-step transition between any given pair of locationd. We fill in this matrix by looping through all the actions. For each action, we: \n","\n","  - enumerate all possible starting rows and columns,  `i1` and `j1`\n","  - define `i-next` and `j-next` to encode the location that results from adding an action’s row and column increments to the current row and column, then applying `clip` to keep the results in the range of the grid\n","  - loop through all the possible next locations, row `i2` and column `j2`\n","  - fill in the transition matrix with a 1 if the new `i2 == i-next` and `j2==j-next`\n","\n"]},{"cell_type":"code","metadata":{"id":"T7kjfEnjGNvs","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593945459690,"user_tz":240,"elapsed":625,"user":{"displayName":"Michael Littman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh1OI1zp1NevMSZ87UN47k8bTZ--Ijr4v3ORV33Vg=s64","userId":"06751891446459829367"}}},"source":["# r is the rewards for the different location categories\n","r = np.array([0, -1, -1, -1, 10])\n"," \n","matmap = np.zeros((nrows,ncols,ncats))\n","for i in range(nrows):\n","  for j in range(ncols):\n","    for k in range(ncats):\n","      matmap[i,j,k] = 0+(grid_map[i,j] == k)\n","def clip(v,min,max):\n","  if v < min: v = min\n","  if v > max-1: v = max-1\n","  return(v)\n"," \n","acts = [(-1,0), (0,1), (1,0), (0,-1), (0,0)]\n","nacts = len(acts)\n","mattrans = np.zeros((nacts,nrows*ncols,nrows*ncols))\n","for acti in range(nacts):\n","  act = acts[acti]\n","  for i1 in range(nrows):\n","    for j1 in range(ncols):\n","      inext = clip(i1 + act[0],0,nrows)\n","      jnext = clip(j1 + act[1],0,ncols)\n","      for i2 in range(nrows):\n","        for j2 in range(ncols):\n","          mattrans[acti,i1*ncols+j1,i2*ncols+j2] = 0+((i2 == inext) and (j2 == jnext))"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VikRoa-DVd7v","colab_type":"text"},"source":["We make a keras variable `rk` for the category rewards. Then, we take `matmap` and compute the dot product of `matmap` with `rk` to map each location to its reward value.\n","\n","We use these values as the starting point for our reinforcement-learning process, assigning it to `v`, the robot’s estimated best value for being in each location.\n","\n","We calculate how much to discount future reward compared to current reward with the parameter `gamma`.  When `gamma = 0.9`, a reward retains 90 percent of its value one time step later. When `gamma` is close to 1, the robot cares a lot about making choices that will eventually lead to high reward. When `gamma` is close to zero, the robot only cares about short term gain, even if that means sacrificing opportunities for additional rewards later.\n","\n","We also define `beta`, which indicates how \"carefully\" the robot will choose among closely valued actions. High values of `beta` cause the robot to reliably choose the highest valued actions and  values close to zero result in the robot being more “careless” and choosing randomly among the available options. `beta = 10` results in moderately careful reward maximizing choices."]},{"cell_type":"code","metadata":{"id":"ZA_bb1U4GSQ2","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593945463203,"user_tz":240,"elapsed":538,"user":{"displayName":"Michael Littman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh1OI1zp1NevMSZ87UN47k8bTZ--Ijr4v3ORV33Vg=s64","userId":"06751891446459829367"}}},"source":["rk = K.placeholder(len(r))\n","rfk = K.dot(K.constant(matmap),K.reshape(rk,(-1,1)))\n","rffk = K.reshape(rfk,(-1,1))\n"," \n","v = K.reshape(rfk,(-1,1))\n","gamma = 0.90\n","beta = 10.0"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dRr_6Llsbf6h","colab_type":"text"},"source":["The robot will look 50 steps into the future when it makes decisions. That’s a reasonable choice as the robot can make it across the entire grid world in 15 steps.\n","\n","For each step of looking into the future, the robot considers each of the five action calucates an estimate of the value of taking each action.  Each value of `q0`, `q1`, `q2`, `q3`, and `q4` corresponds to the estimated value of actions 0 to 4, by multipling the transition matrix for an action by the value estimate, `v`.\n","\n","`Q` brings all those values together into one array. The array has one row for each location and one column for each action.\n","\n","`pi` represents the robot’s assigned probability to each action in each location. The probability of the robot taking an action in a location is determined by how much reward `Q` predicts it will get from taking that action in that location. The higher the predicted reward, the more likely the robot is to take that action. `beta` controls how the strength of the relationship between reward and probability.\n","\n","The value `v` reflects how good it is to be in each location looking ahead one additional step. The planner multiplies the action matrix times the values, discounted them by gamma, and then added in the reward for being in the current state."]},{"cell_type":"code","metadata":{"id":"Q7AfS5haGSUI","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593945469715,"user_tz":240,"elapsed":1361,"user":{"displayName":"Michael Littman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh1OI1zp1NevMSZ87UN47k8bTZ--Ijr4v3ORV33Vg=s64","userId":"06751891446459829367"}}},"source":["for _ in range(50):\n","  q0 = K.dot(K.constant(mattrans[0]),v)\n","  q1 = K.dot(K.constant(mattrans[1]),v)\n","  q2 = K.dot(K.constant(mattrans[2]),v)\n","  q3 = K.dot(K.constant(mattrans[3]),v)\n","  q4 = K.dot(K.constant(mattrans[4]),v)\n","  Q = K.concatenate([q0,q1,q2,q3,q4])\n","  pi = K.softmax(beta*Q)\n","  v = rffk + gamma * K.reshape(K.sum(Q * pi,axis=1),(-1,1))"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rIMtOowGbgtG","colab_type":"text"},"source":["We define the `planner` function in terms of the reward `rk`, our policy `pi`, and our valeus for each action `Q`, we then run the planner to get our final values of the policy `piout` and values `Qout`."]},{"cell_type":"code","metadata":{"id":"XtlvkpJ7GSWz","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593945476479,"user_tz":240,"elapsed":2063,"user":{"displayName":"Michael Littman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh1OI1zp1NevMSZ87UN47k8bTZ--Ijr4v3ORV33Vg=s64","userId":"06751891446459829367"}}},"source":["planner = K.function([rk], [pi, Q])\n","\n","r = np.array([0, -1, -1, -1, 10])\n","piout, Qout = planner([r])"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EEvV5piKbhcR","colab_type":"text"},"source":["We also define the function `plotpolicy` which uses `findpol` to visualize the policy's learned trajectories given the grid setup."]},{"cell_type":"code","metadata":{"id":"BZv26xWxaKpD","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593945479973,"user_tz":240,"elapsed":559,"user":{"displayName":"Michael Littman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh1OI1zp1NevMSZ87UN47k8bTZ--Ijr4v3ORV33Vg=s64","userId":"06751891446459829367"}}},"source":["def findpol(grid,pi,r,c):\n","  if grid[r][c] != 6: return\n","  maxprob = max(pi[r*ncols+c,:])\n","  a = 6\n","  for ana in range(5):\n","    if pi[r*ncols+c, ana] == maxprob: a = ana\n","  grid[r][c] = a\n","  r += acts[a][0]\n","  c += acts[a][1]\n","  findpol(grid,pi,r,c)\n","\n","def plotpolicy(pi):\n","  grid = []\n","  for r in range(nrows):\n","    line = []\n","    for c in range(ncols):\n","      line += [6]\n","    grid += [line]\n","  findpol(grid,pi,0,0)\n","  for r in range(nrows):\n","    line = \"\"\n","    for c in range(ncols):\n","      line += '^>v<x? '[grid[r][c]]\n","    print(line)"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QbTmpF3oryx9","colab_type":"text"},"source":["Here, we learn a `loss` function based on a trajectory of actions `trajacts`.\n","\n","We convert this sequence of actions into a sequence of locations, `trajcoords`, using `reduce`.\n","\n","Our trajectory here goes down the grid, then to the right all the way to green. Then, it backtracks two spots to a yellow square and stays there."]},{"cell_type":"code","metadata":{"id":"9seA4m0ZkDAn","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593945808149,"user_tz":240,"elapsed":824,"user":{"displayName":"Michael Littman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh1OI1zp1NevMSZ87UN47k8bTZ--Ijr4v3ORV33Vg=s64","userId":"06751891446459829367"}}},"source":["from functools import reduce\n"," \n","#  0      1     2     3     4\n","# up, right, down, left, stay\n"," \n","# trajacts = [1,1,2,2,2,2,1,1,1,1,1,1,1,4,4,4,4,4,4,4]\n","trajacts = [2,2,2,2,1,1,1,1,1,1,1,1,1,3,3,4,4,4,4,4,4,4,4,4]\n"," \n","trajcoords = reduce((lambda seq, a: seq+[[seq[len(seq)-1][0] + acts[a][0], seq[len(seq)-1][1] + acts[a][1]]]), trajacts, [[0,0]])"],"execution_count":16,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4fYHLwSdrxv0","colab_type":"text"},"source":["To compute the loss, we run through this sequence of actions and look at the policy `pi`. We find the probability assigned to the the action, sequence pair at timestep `i` and take the negative log of that value and add it to the `loss`."]},{"cell_type":"code","metadata":{"id":"JvN6tKUzkDD3","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593945810286,"user_tz":240,"elapsed":563,"user":{"displayName":"Michael Littman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh1OI1zp1NevMSZ87UN47k8bTZ--Ijr4v3ORV33Vg=s64","userId":"06751891446459829367"}}},"source":["loss = 0\n","for i in range(len(trajacts)):\n","  acti = trajacts[i]\n","  state = trajcoords[i]\n","  loss += -K.log(pi[state[0]*ncols+state[1]][acti])"],"execution_count":17,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aEZ7Ue6frw60","colab_type":"text"},"source":["We use gradient descent to search for a reward function that produces low loss. We pick a `learning_rate` and randomly initialize our values for the reward `r`.\n","\n","We compute the gradient of the loss with respect to the rewards, which we store in the variable, `grads`. Next, we create a keras function that takes the rewards, `rk` as input and produces the `loss` and the `grads` as output, `iterate`.\n","\n","Over 5000 steps, we will update `r` based in the `learning_rate` and `grads_value` of the gradient.  We also will print our `loss_value` every 100 steps."]},{"cell_type":"code","metadata":{"id":"1IHk5wZWkDG_","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":867},"executionInfo":{"status":"ok","timestamp":1593945835088,"user_tz":240,"elapsed":21546,"user":{"displayName":"Michael Littman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh1OI1zp1NevMSZ87UN47k8bTZ--Ijr4v3ORV33Vg=s64","userId":"06751891446459829367"}},"outputId":"99c47e85-8320-4614-c0f5-e6e9dba52e3a"},"source":["learning_rate = 0.001\n"," \n","r = np.random.rand(5)*2-1\n"," \n","grads = K.gradients(loss, rk)[0]\n","iterate = K.function([rk], [loss, grads])\n"," \n","for iter in range(5000):\n","  loss_value, grads_value = iterate([r])\n","  r -= learning_rate * grads_value\n","  if iter % 100 == 0: print(loss_value, r)"],"execution_count":18,"outputs":[{"output_type":"stream","text":["66.92691 [-0.30179562 -0.08337365 -0.04981262 -0.13196386 -0.24937339]\n","27.00261 [-0.18404132 -0.41331831 -0.30594158  0.05634261  0.03064293]\n","26.961533 [-0.16411609 -0.43974154 -0.34664311  0.07977303  0.05441296]\n","26.949188 [-0.15287236 -0.45798304 -0.3663168   0.09306403  0.06779098]\n","26.943497 [-0.14539594 -0.47359789 -0.37621241  0.10206255  0.07682984]\n","26.940262 [-0.1400663  -0.48725084 -0.38083215  0.10852442  0.0833149 ]\n","26.93816 [-0.13608736 -0.49923664 -0.38247572  0.11334496  0.08814898]\n","26.936657 [-0.13299798 -0.50980865 -0.38242962  0.11706169  0.09187538]\n","26.935528 [-0.13051636 -0.51919301 -0.38144464  0.12001867  0.09483879]\n","26.934643 [-0.12846068 -0.52758276 -0.37996536  0.12244199  0.09726576]\n","26.933918 [-0.12670728 -0.53513706 -0.37825463  0.1244861   0.09931284]\n","26.933338 [-0.1251786  -0.54198519 -0.37646484  0.1262501   0.10107933]\n","26.932846 [-0.12382128 -0.54823227 -0.37468373  0.12780331  0.10263419]\n","26.932426 [-0.12259618 -0.55396362 -0.37295799  0.12919465  0.10402648]\n","26.93208 [-0.12147816 -0.55924901 -0.37131065  0.13045533  0.10528872]\n","26.931778 [-0.12044873 -0.56414604 -0.36975105  0.13161094  0.10644378]\n","26.931522 [-0.11949376 -0.56870267 -0.36828138  0.13267681  0.10751079]\n","26.931293 [-0.11860097 -0.57295896 -0.36689803  0.13366814  0.10850316]\n","26.93108 [-0.11776447 -0.57694872 -0.36559641  0.13459433  0.10942983]\n","26.930912 [-0.11697449 -0.58070076 -0.36437032  0.1354664   0.11030182]\n","26.930752 [-0.11623109 -0.58423971 -0.36321433  0.1362837   0.11111969]\n","26.930609 [-0.11552533 -0.58758692 -0.36212268  0.13705756  0.11189378]\n","26.930481 [-0.11485346 -0.59076071 -0.36108955  0.13779218  0.11262883]\n","26.93037 [-0.11421257 -0.59377714 -0.36011014  0.13849215  0.11332876]\n","26.930271 [-0.11359991 -0.59665009 -0.35917907  0.13915905  0.11399537]\n","26.930183 [-0.11301408 -0.59939191 -0.35829253  0.13979557  0.11463203]\n","26.930084 [-0.11245226 -0.60201356 -0.35744736  0.14040442  0.11524134]\n","26.930008 [-0.11191242 -0.60452465 -0.35663985  0.14098888  0.11582521]\n","26.929935 [-0.11139485 -0.60693377 -0.35586746  0.14154772  0.11638408]\n","26.929874 [-0.11089656 -0.6092487  -0.35512783  0.14208598  0.11692219]\n","26.929796 [-0.11041359 -0.61147625 -0.35441792  0.14260415  0.11744156]\n","26.929745 [-0.10994477 -0.61362257 -0.35373497  0.14310819  0.1179457 ]\n","26.929684 [-0.10949474 -0.6156931  -0.35307696  0.14359118  0.11842837]\n","26.929634 [-0.10905967 -0.61769314 -0.35244374  0.14405616  0.11889457]\n","26.929592 [-0.10863831 -0.61962722 -0.35183324  0.14450859  0.11934575]\n","26.92955 [-0.10822854 -0.62149948 -0.35124359  0.144946    0.11978337]\n","26.929508 [-0.10783125 -0.62331374 -0.35067368  0.14537066  0.12020761]\n","26.929478 [-0.10744544 -0.62507344 -0.35012217  0.14578183  0.12061841]\n","26.929428 [-0.1070684  -0.6267818  -0.34958777  0.146183    0.12102046]\n","26.92939 [-0.10670347 -0.62844166 -0.34906953  0.146571    0.12140863]\n","26.929367 [-0.106347   -0.6300558  -0.34856682  0.1469507   0.12178728]\n","26.929329 [-0.10600065 -0.63162666 -0.34807881  0.14731762  0.12215511]\n","26.92931 [-0.10566183 -0.63315656 -0.34760448  0.14767758  0.12251488]\n","26.929268 [-0.10533181 -0.63464757 -0.34714297  0.14802617  0.12286389]\n","26.92926 [-0.10500922 -0.63610172 -0.34669409  0.14836843  0.12320561]\n","26.929226 [-0.10469523 -0.63752078 -0.34625683  0.14870026  0.12353707]\n","26.929201 [-0.1043873  -0.6389065  -0.34583097  0.14902591  0.12386327]\n","26.929184 [-0.10408531 -0.6402604  -0.34541548  0.14934505  0.12418201]\n","26.929153 [-0.10379039 -0.64158392 -0.34500944  0.14965583  0.1244928 ]\n","26.929138 [-0.10350173 -0.64287853 -0.34461345  0.14996038  0.12479795]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"F8CG3hPclpkC","colab_type":"text"},"source":["The resulting reward function assigns small penalty for white (to keep things moving), a big penalty for blue and a moderate penalty for orange. Yellow and green get rewards, with yellow's larger than green's.\n","\n","The optimal trajectory with this reward function goes straight to yellow and stays there."]},{"cell_type":"code","metadata":{"id":"xJKmV9qRj672","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":102},"executionInfo":{"status":"ok","timestamp":1593945880271,"user_tz":240,"elapsed":558,"user":{"displayName":"Michael Littman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh1OI1zp1NevMSZ87UN47k8bTZ--Ijr4v3ORV33Vg=s64","userId":"06751891446459829367"}},"outputId":"89b9ff0b-25ec-4c06-d4d4-99dbf0309b30"},"source":["# r = np.array([ 0.2269183,  -0.63973874,  0.08437171,  0.21648399,  2.07434295])\n","r = np.array([-0.10350173, -0.64287853, -0.34461345,  0.14996038,  0.12479795])\n","piout, Qout = planner([r])\n","plotpolicy(piout)"],"execution_count":19,"outputs":[{"output_type":"stream","text":[">>>>v     \n","    >>>v  \n","       v  \n","       v  \n","       x  \n"],"name":"stdout"}]}]}
