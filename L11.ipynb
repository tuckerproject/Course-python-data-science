{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"L11.ipynb","provenance":[],"collapsed_sections":[]},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"oJVOsD5TDbPK","colab_type":"text"},"source":["In this notebook, we will write our own k-means model from scratch and use it to cluster handwritten numbers from the MNIST dataset\n","\n","\n","In the cell below, we create an `assign_data` function, which takes the `data` and the `centers` for each cluster and makes an assignment of each datapoint in `data` to the closest of the `centers`, `centerids`.  We extract `n`, the number of datapoints, `d`, the dimensionality of the datapoints, and `k` the number of centers.\n","\n","Next, we need to compute the squared distance between each center and each data point.\n","\n","Reshaping the data to be 1 x `n` x `d`, and the centers to be `k` x 1 x `d` signals to numpy that when it subtracts these two arrays, it creates an array of shape `k` x `n` x `d`. That is, it computes all combinations of the `k` centers and the `n` datapoints for each of the `d` dimensions. We assign those differences to `res`.\n","\n","Squaring each of the differences, then summing along dimension 2 --- that’s the components of the vectors --- produces the sum of squared distances, which is the squared distance between the centers and the datapoints.  The resulting array is of shape `k` x `n`.\n","\n","`assign_data` also computes the `loss`, which the sum of the squared differences. We want to know which center has the *smallest* squared distance for each data point. `argmin` produces the index of an array with the smallest value along the given dimension. Here, we’re using dimension 0, which varies over the `k` centers. `centerids` is now an array with one integer for each datapoint that indicates which of the centers is closest.\n","\n"]},{"cell_type":"code","metadata":{"id":"QtZgh0QsJpid","colab_type":"code","colab":{}},"source":["def assign_data(data,centers):\n","  # n is the number of data points\n","  n = len(data)\n","  # d is the dimensionality of the data points\n","  d = len(data[0])\n","  # k is the number of clusters\n","  k = len(centers)\n","  # first, subtract the set of centers from each data point\n","  res = np.reshape(data,(1,n,d))-np.reshape(centers,(k,1,d))\n","  # sum the squared differences\n","  res2 = np.add.reduce(res**2,2)\n","  # assign each data point to its closest center\n","  centerids = np.apply_along_axis(np.argmin,0,res2)\n","  # While we're here, make a note of the loss\n","  loss = sum(np.apply_along_axis(np.min,0,res2))\n","  return(centerids, loss)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2x4tp5vkDqDh","colab_type":"text"},"source":["Next we'll compute the mean of each of the `k` centers using the `data` and their `centerids` assignments. `compute_means` takes the data and the center ids and computes the centers by averaging all of the datapoints with the same id.  This will be used to update the `centers`. \n","\n","After extracting the number of datapoints and dimension, we initialize the array of center locations to a `k` x `d` array of all zeros.\n","\n","For each of the cluster id values from 0 to `k`, we do the following operations:\n"," - First, form a smaller array, `cols`, consisting of all the datapoints with the current center id.\n"," - To be robust, we make sure `cols` has a length greater than zero. That can happen if there’s a center that has been elbowed out of the running by the other centers being closer to all of the data points. \n","  - If it equals zero, that means our center is out of the action and we should probably pick a different location for it. We simply choose one of the data points at random to be this new location.\n","\n","- We want to move the center to the `mean` of the closest points. Numpy’s `mean` method computes the average of an array along any given dimension. Here, we choose dimension 0, which corresponds to the different data points. `mean` produces a component-wise average of all the data points with cluster id equal to `i`.\n","- After completing the loop, we return the newly computed `centers`.\n"]},{"cell_type":"code","metadata":{"id":"vifay3EeJqi-","colab_type":"code","colab":{}},"source":["def compute_means(data, centerids, k):\n","  # n is number of data points\n","  n = len(data)\n","  # d is dimensionality of the data points\n","  d = len(data[0])\n"," \n","  # Zero out the centers\n","  centers = np.zeros(shape=(k,d))\n"," \n","  # loop over the clusters\n","  for i in range(k):\n","    # Gather the data points assigned to cluster i\n","    cols = np.array([data[j] for j in range(n) if centerids[j] == i])\n","    # Average to get mean for that cluster\n","    if len(cols) == 0: \n","      centers[i] = data[random.randint(0,n-1)]\n","    else:\n","      centers[i] = cols.mean(0)\n","  return(centers)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5-qbkig5D29K","colab_type":"text"},"source":["With these two functions, we can build a `kmeans` model, which takes in the `data` and number of clusters, `k` and iteratively builds `k` clusters and updates them relative to the `loss`.  \n","\n","We initialize the `k` centers by selecting random data points. We loop until the `loss` stops changing. If `oldloss` is different from the new `loss`, we use `assign_data` to assign each datapoint to its closest center. Then, we use `compute_means` to move the centers to the means of the points assigned to them. We repeat until the `loss` stops changing, returning the final `loss` and `centers`.  \n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"Ex_bJQGsJql8","colab_type":"code","colab":{}},"source":["def kmeans(data, k):\n","  n = len(data)\n","  d = len(data[0])\n","  # grab the centers from random points\n","  centers = data[[random.randint(0,n-1) for i in range(k)]]\n","  oldloss = 0\n","  loss = 1\n","  while oldloss != loss:\n","    oldloss = loss\n","    centerids, loss = assign_data(data,centers)\n","    centers = compute_means(data, centerids, k)\n","  return(centers, loss)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zxwnWjHWD7Cd","colab_type":"text"},"source":["We will download the MNIST dataset and split the data into training data, `X_train` and `y_train` and test data, `X_test` and `y_test`."]},{"cell_type":"code","metadata":{"id":"DZohJqLrJqoS","colab_type":"code","outputId":"9f8c8492-7d3d-4c15-b6eb-9c5e3309624e","executionInfo":{"status":"ok","timestamp":1590502090548,"user_tz":240,"elapsed":25263,"user":{"displayName":"JZ Forde","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiwSogjDlg0BxeyyCUFJ3t-a6Mt210NSXjrBuLMYw=s64","userId":"11263008094098858557"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from sklearn.datasets import fetch_openml\n","data = fetch_openml(name='mnist_784')\n","\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.1)\n","X_train, X_test, y_train, y_test = train_test_split(X_test, y_test, test_size=0.33)\n","len(X_train)"],"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["4690"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"markdown","metadata":{"id":"NsiSIN2rELNf","colab_type":"text"},"source":["Here, we run `kmeans` on our `X_train` data where `k=10`.  We run `kmeans` 9 times and record the `bestcenters` which have the `bestloss` among the recorded losses.  We then find the accuracy of these new centers on the test set."]},{"cell_type":"code","metadata":{"id":"85T_3_oHKa31","colab_type":"code","outputId":"ef60f19f-5c61-4e91-a656-87d512766ab8","executionInfo":{"status":"ok","timestamp":1590502210859,"user_tz":240,"elapsed":120302,"user":{"displayName":"JZ Forde","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiwSogjDlg0BxeyyCUFJ3t-a6Mt210NSXjrBuLMYw=s64","userId":"11263008094098858557"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["from scipy import stats\n","import math\n","from functools import reduce\n","import random\n","\n","#for nlabeled in range(20,len(X_train),10):\n","nlabeled = 20\n","if True:\n","  print(nlabeled)\n","  ans = []\n","  k = 10 # 2 # 5 # 20\n","  if True:\n","    bestcenters, bestloss = kmeans(X_train, k)\n","    for rep in range(9):\n","      centers, loss = kmeans(X_train, k)\n","      if loss < bestloss: bestcenters, bestloss = centers, loss\n","    # How do we test the clustering that was discovered?\n","    # Assign testing points to clusters\n","    test_centerids, loss = assign_data(X_test, bestcenters)\n","\n","    # Use the labeled examples to label the clusters\n","    train_centerids, loss = assign_data(X_train[:nlabeled], bestcenters)\n","    #print(train_centerids)\n","    #print(y_train[:nlabeled])\n","    labs = y_train[:nlabeled]\n","\n","#    clust_labs = np.zeros(shape=(k))\n","    clust_labs = np.repeat(labs[0],k)\n","    for i in range(k):\n","      mode = stats.mode(labs[train_centerids == i]).mode\n","      if len(mode) > 0: clust_labs[i] = mode[0]\n","\n","# print(clust_labs)\n","    ans = ans + [(k,sum(clust_labs[test_centerids] == y_test)/len(y_test))]\n","#    plt.plot(X_test[clust_labs[test_centerids] == 0,0],X_test[clust_labs[test_centerids] == 0,1],'o',color='r')\n","#    plt.plot(X_test[clust_labs[test_centerids] == 1,0],X_test[clust_labs[test_centerids] == 1,1],'o',color='b')\n","#    plt.show()\n","\n","#  print(ans)\n","  print(reduce((lambda x, y: x if x[1] > y[1] else y), ans))\n","  labids, loss = assign_data(X_test, X_train[:nlabeled])\n","  print(nlabeled, sum(y_train[labids] == y_test)/len(y_test))"],"execution_count":14,"outputs":[{"output_type":"stream","text":["20\n","(10, 0.47056277056277057)\n","20 0.48917748917748916\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"3iz6oNKK8Tux","colab_type":"text"},"source":["We'll next print the images that best represent the centers of each of our clusters in K-means and the label for each of the clusters\n","\n","We will also calcuate the percent accuracy of the clusters"]},{"cell_type":"code","metadata":{"id":"er8dfhHEJqrA","colab_type":"code","outputId":"2b6009cb-d25e-4349-c162-91c23b76443d","executionInfo":{"status":"ok","timestamp":1590502234267,"user_tz":240,"elapsed":5964,"user":{"displayName":"JZ Forde","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiwSogjDlg0BxeyyCUFJ3t-a6Mt210NSXjrBuLMYw=s64","userId":"11263008094098858557"}},"colab":{"base_uri":"https://localhost:8080/","height":620}},"source":["!pip install keras=='2.3.1'\n","from keras.preprocessing.image import array_to_img\n","\n","train_centerids, loss = assign_data(X_train, bestcenters)\n","test_centerids, loss = assign_data(X_test, bestcenters)\n","\n","clust_labs = np.repeat(labs[0],k)\n","for i in range(len(bestcenters)):\n","  display(array_to_img(np.reshape(bestcenters[i],(28,28,1)), scale=False))\n","  clust_labs[i] = y_train[train_centerids == i][0]\n","  print(clust_labs[i])\n","#  mode = stats.mode(y_train[train_centerids == i]).mode\n","#  print(mode[0])\n","#  if len(mode) > 0: clust_labs[i] = mode[0]\n","\n","sum(clust_labs[test_centerids] == y_test)/len(y_test)"],"execution_count":15,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: keras==2.3.1 in /usr/local/lib/python3.6/dist-packages (2.3.1)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras==2.3.1) (1.12.0)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras==2.3.1) (3.13)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras==2.3.1) (1.1.2)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras==2.3.1) (2.10.0)\n","Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras==2.3.1) (1.18.4)\n","Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras==2.3.1) (1.4.1)\n","Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras==2.3.1) (1.0.8)\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABpklEQVR4nL3SXW/aMBQG4PfYTpyk0JaWsqrTRj+mbVXVi/3//zBxMY1ptGWaOqgGJAUCIcH26UX4qNj9fGWfR6/fCxv4/4t2DgwAJJh5B4lIgFgISGELw1DlXAgIpXwd+Eq6IlvawtoSiaTy/WC/3jisKCnNc38wHC+AEoXeq1QbzaszXWRWe5MI8zEEswIgPB3VTi4/X6jB09hvHKoD5SyvkhCA07WD7OHuKTu6PaZ8OimMKzudNYULwvn914ep9IIgjkdp7hwgALApjPRd/1tnlEfvmtG0H2eba9k5qTBLxl4t+PDlPPkznBvedLIAFiOz71N4exN9/xXnRMQrJLL5s/W0j/r16fDucWJ4mwTbxOyFQlROmvSz3ZsZy9igXbh84kldvaj1Wp3RwrotOlfYTCrdePu+aLUeZ2VunXRkYG1wdn183+qmZmUQZadjiPDNp8tZuz3c2BqZpK5ffQy6P3rFxlYIhqqenteSTneytfVjE+nqUWWe/P67xC4SvCDy80HcS43jHWRJWMYuSPvpK1t/MKm8MPJVno6X7h9c7Qn8KogXLo/WWOw0UY8AAAAASUVORK5CYII=\n","text/plain":["<PIL.Image.Image image mode=L size=28x28 at 0x7F8DA0055EF0>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["7\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABnklEQVR4nL2QS2/aUBBGZ+71ExPjGOziSK1QNn2ou/7/X9BFl62U0FBSYqc2OAb8ur6PLsDE+0r9ViMdnZlPA/D/g8OZ6JZtQNe0XEg5hEhtL3x3E47UPn5M8rJpJGhnRgw3ev/pQ2gSWWZ398vngnfqDIk+jj5/+XgtqhI0a5qO9weN9xC1Ubh4Cz9/p409c2vecQ5wXotInMlVt/q+ftHnJux2RSkEADmZipimeonXyy26TpM95xXj6lJISuBCwMi9XcDuKTnUneyhkqJjgoZNFMz55mG9q5hQ8GqyutWDN8qo75d3ScmluhQCJasi55NrWq+yVXLkEgCGZnXcM46yYeL0uwFE5C3LSkTNG+sKhhCJYTkWfzqIKJi5Gg4gItUcb+axTUqd+ZVjEDy5BACQ6mN/6pvpOuaUIiG9SQAQqe2G8xmthXd7A0XJBzcVEGJPoqlhR/7C28Y5g/NaDQAk54xbge/otto+rNJKqhMlAKBEe8jiXHkBPn79tvzTSng1QbaFaNIfvnnc/Eq2jexvXopRqqESQkrVo3/JX5ot3dOOFX+qAAAAAElFTkSuQmCC\n","text/plain":["<PIL.Image.Image image mode=L size=28x28 at 0x7F8DA0055DA0>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["8\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABp0lEQVR4nLXS3W7aQBAF4JmdXRvbgH+AAAmRetX3f5m+QVESkRpiYq/t9c70wilYvazUuVrp0+7OkQ7Avw7eTwioQBhE/kbURgdxSIbr1rYDywQxjOf5drtZBsP78efx0g4AAHrEcFFsnr89befUXHcxeu/lhjpaZg+7PBFLoebWVk13QzRhEKq6rJ2YdB0Vq7Rs+IbIzn64M9RAm+/PcWzGVcZnfddQR4qtm7kDGebhjsI9DIawdwp1ANI3ju832XeOFIrSySqR1tpBJlHEs0cVzPLHVYTXc+MnKKAU6VmSbg/7+LP8Zf1kITQmSubxMl8/bcLzWzUAif8TJUizNC3WebqYJfZSMoVuQAENAKizx+3DepUvY2X6zupoQX3DI6pofTjs5onRFLJnFS28adgzaACg+WZ/KDShKGRQWeWCN0c4/mnCZLUvPKDG5moHUwC1rZIRWflosVQD91V56pEcexbkEb2tLx+B9vXpeLr2RKYuL5/2K6evX37UmeHq9aXqkAhdY62Tr5qYOMkS1V+rznkgI4Cu85MOASIwyNhCELg18D/Mb3hGzhgcaEoEAAAAAElFTkSuQmCC\n","text/plain":["<PIL.Image.Image image mode=L size=28x28 at 0x7F8DA0055B00>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["2\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABxklEQVR4nLXS21PaQBTA4bO7SciFCCZFhQwZlNrRUksvjv3/XzrtC7S22k5FrSggFwlIDbntnj7UCJ32qTM9j/vNb3cfDsC/DvnrEf6JBIARSpAAF4i4hIQQIht63lL57Hb8I4kFgrQIKcsVS66rxjfti+4EI7hHggCMZSuPdyqrWeEVsgzjMEUEQml269XzqgbA9DLOJyMq0msJAbn45qAqBeOxUIlumRLC4k1lY39/R+q3zmeZdYtoBl1CuVA/2FV6Hz5fRitcVonC4AFZbvPlXvay0biYItwlRMIFMsOp7+X7nw7b00gIpgKPRIpUe7RZ2xgcN1seB9XImUkQcACgv7D8tBSeN79e+xw0p2xyz4/TUrYrZaV79G0YUKFaboVOvEmSltp6aeXu+0k/BlTs2rO1+bB7G92XRHcsbdQZ+aBqVu31k3m3dT1LP4RS3lIQDZMahe0Xden4Y+smgrQEppl2MVjN2G7VJUdvG8MYMP1QMA6hmHFC1c5ps7N37zu+gLQE/9TVS8W1BMS8c3L4pecn/AExvGrG27aeBNPu6VnbCxOxtENMzjtbJSMe9waDacA5/rZ9lEqKKkESz2NOUvpv8xNAwcTLjScWwAAAAABJRU5ErkJggg==\n","text/plain":["<PIL.Image.Image image mode=L size=28x28 at 0x7F8DA00551D0>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["0\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABmklEQVR4nL2SbW/aQBCEZ/bOb3FAlABq1EitUvX//6GqqtQ0ygsQDBgbfLfbDzapkPq5++30aGb3RgP8/+HlyyiEmcH+QlKckDSTNEETgkYD4AGAkmRpml+NHAShqQ71UWE9pEuL68lsPk2hIu3rwyrEqDB4gC4pxvO7+1upN0c/zSahqVuyV5I+Gy3uv6Uvz4/NtdwtXl/WwvedkhSzmV9+/7nqbuaS3RRQs2En4NIi2//68XsvhfrUB41qAAQwNSSe29Vy06K4yhkCzNDbmpm5zInk02Ty5W5SdWYEz7ZRo7nxJ3bZ6PZjeTCQogO0eKqrXbkYmZNxWa6VdBxsYbHdPvhp4fV4orjQdgqC1ttq+4Z6lks48fPXulrvj8HO8UHDtqsePWDFh4ZPq32n52sBBA17ISlzH6plHQG8JwQoDKAkecHN9mTWf7SHBgUI+HHZ7Q4BQwgydMBIQMpJ3jXhLDzb9m3IR6VVu6OakXYBQeekWdpT1XSquFAqEU718ylUb9vDQC/aJ1mZxbYLMdo/qinOdKglAPwBBvHf7/UAgnYAAAAASUVORK5CYII=\n","text/plain":["<PIL.Image.Image image mode=L size=28x28 at 0x7F8DA0055C50>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["9\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABmElEQVR4nK2SQW/TQBCFZ2Z3HccmDklblEQQqoKQ+v9/DFKQSCikqJCmJKRx7F3P68Gxabhw4V328GnmvX0aov8q/uslImYmAhELAD2BLCLCFPUzPeT7fQCIbLPAWGZhl11NdHW3ksIHPUJmsYaYTTx697aKHndBSZWkWQuQ2GTw5nIyiLqRtdZKDZkAVOBOcjZ5lcEHcZE1qCFAULCJBxfjocnzgydhoAlEBHAy6J8P44e729XOhzK0aUFsuv2sd94rlrPlugyKiv5MSvwypWyaPX6+We9RKYHatOLS1CTvX2er+X0JBYFaKM5F4sbXw/zT95L42FwNRdhQlX4Ymvlsq9x0Wv9TrAiZi3H88PG21BCUQNQEksg6l4175WK+8SEcLZtuO50om0ySzZef3nsSUkabll2SjS5H7sdim3tlObrKsR53Np32N/Pl7xLCBKCFgEeSRsXXxWqvIgzUprUnQtBid5h/+5VXgJ4G0iq/T8TPbrYl0Mw1NyQ27qYvdL0pKhAR4/n1MRtjSUOloGc6OdET8i89AV/ly3UNY9X8AAAAAElFTkSuQmCC\n","text/plain":["<PIL.Image.Image image mode=L size=28x28 at 0x7F8DA00551D0>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["7\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABqklEQVR4nLXSS2/TQBAH8Jnd9e7acR5OpDZq0/JQxYXv/0F6qoALhUJC25TGjuN9D4fGSkDiwIE5rfTT/LUzGoD/Unj8RCAA+hMZEyof6lxxt93tjE0pRgIQAAAs04OTi8XFvMBu/fXb6qFuTeqRy/L07bvX52VGblaNmDMdY+kFkWWD8VB1n33gnPvEgnMhwR4Rkm+Wy3bj1XA06KyzMdEeIUazUb7b1bygTOx2JiSAPVJ0zraCskk+qsrktiYk6juJYvAkJ0Oti4Iets0+9SUWUoxYvlqMpAITlUCCY/Q+SK21FFHOrrrGBDggBFs/Tign4FpfemPt4bcA5JoVfQEb1PzNydn6vvHxeE7zaMB1JFZYqmr6oz0gF4KF1jkfMSxMoQrFsN+tkEpmAl2IEYGY4CiwRybyQV4IHwzjen5aYrKuHwWRy3FV5WH8FMXs6v30ebMx/eIREurqbK5cnfLZfNh8v71v476Tou/qCgfnYwVcuOXN9Yefto8l8A0l58LlVNq724+f7tZdoMMNIZPFqJqU3LZPz7XxRL9fHwJDAACi9A/H+7f6BeMV7I8VmeBXAAAAAElFTkSuQmCC\n","text/plain":["<PIL.Image.Image image mode=L size=28x28 at 0x7F8DA0055C50>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["5\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABTklEQVR4nK2SsW4bMRBEZ4akdJITCLDduE/+/3NSuzEMpIgTH+TTkdxJYd1JTpoU2Yrgw8wudhb4j8W/XwQA0imhN7//5YWJabO73Xt8OUZqH6FS2hzuvzzE47fanYy4QKVc9p/uHr4WPL/MRsRFqVTKdre7ub3nYZdoh1dIJkrsNYKB6N2gF2h0GKXO45zfprnGh2kd0dldJbXacGYLNAAwDwOmYzfPVFipVLY3aaqLboEGbKh83vTaIOkaEhSRt/u9lCVR5MVWInpvhPNQYAK+UgJ2PU3zxM2Q+GdPoLc4vb16s82kJK62ACKsepyczk0vtgYRzJicWkhAwBdbG8xDzgXtFAbo1dYkwGE/oE/HU3P4KjKAElXazx/j3CPCXlMBQbK/Pg3D8/dTOM6xcFmChOFw0K9xrAtbr4+GWAp7rWsoV0cKACLeR/mH+g0kkaxqlk0prgAAAABJRU5ErkJggg==\n","text/plain":["<PIL.Image.Image image mode=L size=28x28 at 0x7F8DA0055B00>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["4\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABr0lEQVR4nLWSy3KbQBBFuxsGEA4ghLBkUUrKSWWR//+L7JPKY2PHdmQLhEEPJBhmOgsbIWWRXXo3c+r28wL8I/D8hcwIAHwOEYEMy7GE4GZX1RoAAMxORIawB348FPvdIlWt7iEiGcJ1h7MkkHljduleICOg5fjzDyNaH57Lg2JABqCupGkNpu8uPZkulkXNL73Qa1uGsEfzid9mv/NKIQACntQUF/EsouxusVYExMB9WjKc8Cp2y7v7Z2XaFgH3SkbTiebR/vFmJYUpUWtG4G5OEt40Nov7XPueaLZiI1s+1qRBOPFkltmjaGTXy/RxfZDHDZEbj+1sJT4mE5/W4aBp2h4KJwhpq5J54gBQW6XlwZQdNAb+G+TAu2yXjXBd2xVEnZJIWLYWQ6coVkYyZNTquCFEZjC8kb0t0sqKQlFtGqVflQyqruR4vFVNkFy/rZ8eVnU/ipZlmk2vrOeDCLzq5tttWUl9hHXxM754P0uUbJ6+f/6yqJu2v6csb3n9ae7j5uHH11/5XqpTD5FhBVHombs839Sy1XzmPgI0hAmq1Vrz0X8nxgQk+vvzP8QfKG7HdwIY6A8AAAAASUVORK5CYII=\n","text/plain":["<PIL.Image.Image image mode=L size=28x28 at 0x7F8DA0055EF0>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["6\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAB0ElEQVR4nKXS32/SUBQH8HPu7YUCK20Fy68NBCdkcfFBn/z/36fJsrkYWSfg3NIBhZaW/rrHB9uIifFBz9NNPjnnJt9zAP618PDNUAIA0R8QhSCkFCGV9Dsi8uozs14RwfIxiCJ5gMg4107G/XY1XTnTmbONMwIABQAAucK103eT7pHc603TuMHtPiuQUNGG78+7ZX/pc7XHOaaJpBwBlcb560F4bztpWe9YtPODtEBEbXzWSxZXUw+YEbw6kd4mlkVnyZqMSvblxUqSEoj68Hjy5CUFVgbjxur6wk4kqvBoNtv920Ug87Hmi1ZiX956BEwyvO92rJa+ypEZlnBt2wXOOITgbvZKowYADABAMbVsOd8CQwTKEj9MeVXF/E9RVeXGkwIQiYiDQBYnlHcyjoIzxhkAScBqU4/XYTGWRQnW9BIREKEwxyPFedjmISD6m6jWblMIwIVx9rbhzua7IiHafWsbI091I1AaL9+MpP3Z2RcJxet5Sz3G507I61a/h1cfpts43wpFyzujNjzt7FKoacK/+Xi93MtiZan3FWXY146kpOD7py93T2F2cCaiYg0GTV1k7sNssd7FEugXIvJSpSxYFu+jNPtJfz/V/6gfRUvYJpbhDHYAAAAASUVORK5CYII=\n","text/plain":["<PIL.Image.Image image mode=L size=28x28 at 0x7F8DA0055DA0>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["0\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["0.45151515151515154"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"markdown","metadata":{"id":"oZjP6jnBgOxB","colab_type":"text"},"source":["Finally, we'll rewrite the K-means model as an active learning problem and perform semi-supervised clustering of the data"]},{"cell_type":"code","metadata":{"id":"Uzl-FUtPHMEx","colab_type":"code","outputId":"077b7d99-8007-4d2e-cd83-c946dbf32c63","executionInfo":{"status":"ok","timestamp":1590499458205,"user_tz":240,"elapsed":694670,"user":{"displayName":"JZ Forde","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiwSogjDlg0BxeyyCUFJ3t-a6Mt210NSXjrBuLMYw=s64","userId":"11263008094098858557"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["from scipy import stats\n","import math\n","from functools import reduce\n","\n","# ACTIVE LEARNING VERSION\n","\n","#for nlabeled in range(20,len(X_train),10):\n","nlabeled = 10\n","if True:\n","  ans = []\n","  k = 50 # 10 # 2 # 5 # 20\n","  if True:\n","#  for k in range(10,200,50):\n","    bestcenters, bestloss = kmeans(X_train, k)\n","    for rep in range(9):\n","      centers, loss = kmeans(X_train, k)\n","      if loss < bestloss: bestcenters, bestloss = centers, loss\n","    # How do we test the clustering that was discovered?\n","    # Assign testing points to clusters\n","    test_centerids, loss = assign_data(X_test, bestcenters)\n","\n","    # Let's label one example in each category\n","    train_centerids, loss = assign_data(X_train, bestcenters)\n","\n","    clust_labs = np.repeat(labs[0],k)\n","    for i in range(len(bestcenters)):\n","      clust_labs[i] = y_train[train_centerids == i][0]\n","\n","    # semi-supervised clustering\n","    print(k,sum(clust_labs[test_centerids] == y_test)/len(y_test))\n","\n","    # nearest neighbors\n","    labids, loss = assign_data(X_test, X_train[:k])\n","    print(k, sum(y_train[labids] == y_test)/len(y_test))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["50 0.664069264069264\n","50 0.5744588744588744\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Yk8rAi6ATrIL","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}
