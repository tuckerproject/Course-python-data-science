{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"L20qs.ipynb","provenance":[],"collapsed_sections":[]},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"k3VNeujCrPBQ","colab_type":"text"},"source":["Our simple yes/no recognizer used a neural network architecture consisting of four convolutional layers and three fully connected layers (over 3 million trainable weights). It got about 96% training and testing accuracy. What do you think would happen if we switch from convolutional layers to solely densely connected layers? A network with one hidden layer of size 200 ends up having a similar number of trainable weights. Make a guess as to what training and testing accuracy you'd see. Run it to find out what happens."]},{"cell_type":"markdown","metadata":{"id":"u-O-IaFDLM3u","colab_type":"text"},"source":["In this notebook we will build a speech recognition model.  \n","\n","Below we'll import the libraries we'll be using."]},{"cell_type":"code","metadata":{"id":"wrWNC6CiH7Dd","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593943212325,"user_tz":240,"elapsed":3347,"user":{"displayName":"Michael Littman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh1OI1zp1NevMSZ87UN47k8bTZ--Ijr4v3ORV33Vg=s64","userId":"06751891446459829367"}}},"source":["import os\n","import librosa   #for audio processing\n","import IPython.display as ipd\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from scipy.io import wavfile #for audio processing\n","import warnings\n","warnings.filterwarnings(\"ignore\")"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VuKhes1QLe3b","colab_type":"text"},"source":["Next, we'll download the dataset of speech commands from tensorflow."]},{"cell_type":"code","metadata":{"id":"LUrISUQjkscm","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":204},"executionInfo":{"status":"ok","timestamp":1593943223560,"user_tz":240,"elapsed":9442,"user":{"displayName":"Michael Littman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh1OI1zp1NevMSZ87UN47k8bTZ--Ijr4v3ORV33Vg=s64","userId":"06751891446459829367"}},"outputId":"4a3c06d6-8dcc-4df5-b87a-b6e83a468f09"},"source":["!wget http://download.tensorflow.org/data/speech_commands_v0.01.tar.gz"],"execution_count":2,"outputs":[{"output_type":"stream","text":["--2020-07-05 10:00:15--  http://download.tensorflow.org/data/speech_commands_v0.01.tar.gz\n","Resolving download.tensorflow.org (download.tensorflow.org)... 173.194.76.128, 2a00:1450:400c:c00::80\n","Connecting to download.tensorflow.org (download.tensorflow.org)|173.194.76.128|:80... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 1489096277 (1.4G) [application/gzip]\n","Saving to: ‘speech_commands_v0.01.tar.gz’\n","\n","speech_commands_v0. 100%[===================>]   1.39G   203MB/s    in 6.9s    \n","\n","2020-07-05 10:00:22 (207 MB/s) - ‘speech_commands_v0.01.tar.gz’ saved [1489096277/1489096277]\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"JZVW2lmaLkHz","colab_type":"text"},"source":["Here, we unzip the file we downloaded from tensorflow."]},{"cell_type":"code","metadata":{"id":"52b5XAo0k1lT","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593943275781,"user_tz":240,"elapsed":57816,"user":{"displayName":"Michael Littman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh1OI1zp1NevMSZ87UN47k8bTZ--Ijr4v3ORV33Vg=s64","userId":"06751891446459829367"}}},"source":["!mkdir speech_commands\n","!tar -C ./speech_commands -xf speech_commands_v0.01.tar.gz "],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4Ve2s44HL6G5","colab_type":"text"},"source":["Below we load the data into `all_wavs` and their respective labels into `all_labs`.  The labels are either `yes` or `no`.\n","\n","We'll also print the number of examples in `all_wavs`."]},{"cell_type":"code","metadata":{"id":"W--i8r_iwMv-","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"executionInfo":{"status":"ok","timestamp":1593943792460,"user_tz":240,"elapsed":299883,"user":{"displayName":"Michael Littman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh1OI1zp1NevMSZ87UN47k8bTZ--Ijr4v3ORV33Vg=s64","userId":"06751891446459829367"}},"outputId":"9db834b5-983f-47ee-8b8e-bbd95a68fc85"},"source":["import os\n","\n","directory = 'speech_commands/'\n","\n","all_wavs = []\n","all_labs = []\n","for label in ['yes', 'no']:\n","    print(label)\n","    wavs = [f for f in os.listdir(directory + label) if f.endswith('.wav')]\n","    for wav in wavs:\n","        samples, sample_rate = librosa.load(directory + label + '/' + wav, sr = 16000)\n","        if(len(samples)== 16000): \n","            all_wavs.append(samples)\n","            all_labs.append(label)\n","print(len(all_wavs))"],"execution_count":4,"outputs":[{"output_type":"stream","text":["yes\n","no\n","4255\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"5OS6-Zk1NwJd","colab_type":"text"},"source":["Below we split our training and test data.  `X_train` is our processed audio files for training and `y_train` are their labels.  `X_test` and `y_test` are our test audio files and their labels, respectively."]},{"cell_type":"code","metadata":{"id":"Mi2-T32hwb6T","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593943796508,"user_tz":240,"elapsed":1311,"user":{"displayName":"Michael Littman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh1OI1zp1NevMSZ87UN47k8bTZ--Ijr4v3ORV33Vg=s64","userId":"06751891446459829367"}}},"source":["from sklearn.model_selection import train_test_split\n"," \n","all_wavs = np.array(all_wavs).reshape(-1,16000,1)\n","all_labs = np.array([lab == 'yes' for lab in all_labs])\n","X_train, X_test, y_train, y_test = train_test_split(all_wavs,all_labs,test_size = 0.2)"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2Hk3xLhZhPLE","colab_type":"text"},"source":["Since we're not using convolutions, we'll just reshape the data to be flat vectors."]},{"cell_type":"code","metadata":{"id":"5e5_NOMD_r7j","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593943966817,"user_tz":240,"elapsed":968,"user":{"displayName":"Michael Littman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh1OI1zp1NevMSZ87UN47k8bTZ--Ijr4v3ORV33Vg=s64","userId":"06751891446459829367"}}},"source":["X_train = np.reshape(X_train, (3404,16000))\n","X_test = np.reshape(X_test, (851,16000))"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qyYmvoBbOcMv","colab_type":"text"},"source":["In the following lines, we will build together the layers of our model for speech recognition."]},{"cell_type":"code","metadata":{"id":"_gLlhz_vy3V6","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":408},"executionInfo":{"status":"ok","timestamp":1593944993555,"user_tz":240,"elapsed":3996,"user":{"displayName":"Michael Littman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh1OI1zp1NevMSZ87UN47k8bTZ--Ijr4v3ORV33Vg=s64","userId":"06751891446459829367"}},"outputId":"3154b5f0-0640-42de-863a-3aa1de5929e8"},"source":["!pip install keras=='2.3.1'\n","from keras.layers import Conv1D, Input, MaxPooling1D, Flatten, Dense\n","from keras.models import Model\n"," \n","inputs = Input(shape=(16000,))\n","\n","x = inputs\n","\n","x = Dense(200, activation='relu')(x)\n","x = Dense(1, activation='sigmoid')(x)\n","\n","outputs = x \n"," \n","model = Model(inputs, outputs)\n","\n","print(model.summary())"],"execution_count":45,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: keras==2.3.1 in /usr/local/lib/python3.6/dist-packages (2.3.1)\n","Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras==2.3.1) (1.18.5)\n","Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras==2.3.1) (1.4.1)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras==2.3.1) (1.12.0)\n","Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras==2.3.1) (1.0.8)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras==2.3.1) (2.10.0)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras==2.3.1) (3.13)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras==2.3.1) (1.1.2)\n","Model: \"model_22\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_22 (InputLayer)        (None, 16000)             0         \n","_________________________________________________________________\n","dense_70 (Dense)             (None, 200)               3200200   \n","_________________________________________________________________\n","dense_71 (Dense)             (None, 1)                 201       \n","=================================================================\n","Total params: 3,200,401\n","Trainable params: 3,200,401\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"IuVQFr34Oj2y","colab_type":"text"},"source":["We then `fit` the model.  We use a `mean_squared_error` `loss` and optimize the weigths using use `adam` as our `optimizer`. We iterate through the data 15 times.  Each time, or `epoch`, we print out the `accuracy` and `loss` of our model so far."]},{"cell_type":"code","metadata":{"id":"oTCjBNo9y8XT","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":561},"executionInfo":{"status":"ok","timestamp":1593945044268,"user_tz":240,"elapsed":13063,"user":{"displayName":"Michael Littman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh1OI1zp1NevMSZ87UN47k8bTZ--Ijr4v3ORV33Vg=s64","userId":"06751891446459829367"}},"outputId":"d93c7b55-f361-40c8-f6eb-fa777f917f5f"},"source":["model.compile(loss='mean_squared_error',optimizer='adam',metrics=['accuracy'])\n","\n","model.fit(X_train, y_train ,epochs=15, batch_size=32)\n","\n","model.evaluate(X_test, y_test)"],"execution_count":46,"outputs":[{"output_type":"stream","text":["Epoch 1/15\n","3404/3404 [==============================] - 1s 274us/step - loss: 0.2643 - accuracy: 0.5235\n","Epoch 2/15\n","3404/3404 [==============================] - 1s 241us/step - loss: 0.1570 - accuracy: 0.7949\n","Epoch 3/15\n","3404/3404 [==============================] - 1s 239us/step - loss: 0.1074 - accuracy: 0.8649\n","Epoch 4/15\n","3404/3404 [==============================] - 1s 240us/step - loss: 0.0773 - accuracy: 0.9083\n","Epoch 5/15\n","3404/3404 [==============================] - 1s 230us/step - loss: 0.0585 - accuracy: 0.9401\n","Epoch 6/15\n","3404/3404 [==============================] - 1s 230us/step - loss: 0.0471 - accuracy: 0.9518\n","Epoch 7/15\n","3404/3404 [==============================] - 1s 234us/step - loss: 0.0390 - accuracy: 0.9624\n","Epoch 8/15\n","3404/3404 [==============================] - 1s 232us/step - loss: 0.0323 - accuracy: 0.9692\n","Epoch 9/15\n","3404/3404 [==============================] - 1s 227us/step - loss: 0.0320 - accuracy: 0.9671\n","Epoch 10/15\n","3404/3404 [==============================] - 1s 231us/step - loss: 0.0385 - accuracy: 0.9621\n","Epoch 11/15\n","3404/3404 [==============================] - 1s 229us/step - loss: 0.0302 - accuracy: 0.9715\n","Epoch 12/15\n","3404/3404 [==============================] - 1s 228us/step - loss: 0.0311 - accuracy: 0.9694\n","Epoch 13/15\n","3404/3404 [==============================] - 1s 231us/step - loss: 0.0205 - accuracy: 0.9809\n","Epoch 14/15\n","3404/3404 [==============================] - 1s 224us/step - loss: 0.0272 - accuracy: 0.9724\n","Epoch 15/15\n","3404/3404 [==============================] - 1s 226us/step - loss: 0.0336 - accuracy: 0.9671\n","851/851 [==============================] - 0s 109us/step\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["[0.38268632302553196, 0.533490002155304]"]},"metadata":{"tags":[]},"execution_count":46}]}]}
